# Language Detection Fix - December 4, 2025

## Problem

WebRTC voice system was responding in **Spanish/Portuguese** instead of English, despite having an English system prompt.

## Root Cause Analysis

### Investigation Process

1. **Examined conversation exports** from `debug/db_exports/voice_conversations/`
2. **Checked system prompt** in `backend/api/realtime_voice.py` (lines 67-103)
3. **Analyzed transcriptions** from recent conversations

### Findings

1. **System Prompt is Correct** ‚úÖ
   - English "Archie" prompt is being sent and confirmed by OpenAI
   - Located at [realtime_voice.py:67-103](backend/api/realtime_voice.py#L67-L103)

2. **Transcription is Working** ‚úÖ
   - Whisper-1 correctly transcribing user speech
   - Detecting multiple languages: English, Portuguese, French, Chinese

3. **Language Auto-Detection Issue** ‚ùå
   - Input transcription had `"language": null`
   - OpenAI was auto-detecting language from audio characteristics
   - Responding in detected language (Portuguese/Spanish) instead of English

### Evidence from Conversation Logs

**User Transcriptions (Mixed Languages):**
```
"Rawr!"                                              # English/playful
"C'est bon, √ßa a l'air bien."                       # French
"Quero saber se voc√™ consegue executar alguma fun√ß√£o." # Portuguese
"‰∏ÄÈªûÈªûÈπΩÂ∑¥"                                          # Chinese
```

**Assistant Responses (Portuguese):**
```
"Ol√°! Que bom falar com voc√™. Pode me dizer no que posso te ajudar hoje?"
"Claro, estou aqui para ajudar..."
"Entendi. Vou verificar as informa√ß√µes..."
```

### Reference Implementation Comparison

The working reference at [refs/webrtc-bridge/src/openai/openai.realtime.ts:56](refs/webrtc-bridge/src/openai/openai.realtime.ts#L56) used a **Portuguese system prompt**:

```typescript
const systemPrompt = 'Voc√™ √© um assistante de voz amig√°vel da TeleChat falando com o usu√°rio pelo navegador.';
```

This explains why previous tests may have worked in Portuguese - the reference was explicitly Portuguese!

### OpenAI Default Behavior

From conversation logs (session.created event):
```json
"instructions": "Your knowledge cutoff is 2023-10. You are a helpful, witty, and friendly AI.
                 Act like a human, but remember that you aren't a human and that you can't do
                 human things in the real world. Your voice and personality should be warm and
                 engaging, with a lively and playful tone. If interacting in a non-English language,
                 start by using the standard accent or dialect familiar to the user..."
```

**Key phrase:** "If interacting in a non-English language, start by using the standard accent or dialect familiar to the user."

This means OpenAI's default behavior is to **match the detected language** when no explicit language constraint is provided.

## Solution

### Code Change

**File:** [backend/api/openai_webrtc_client.py:306-314](backend/api/openai_webrtc_client.py#L306-L314)

**Before:**
```python
if self.enable_input_transcription:
    session_config["input_audio_transcription"] = {
        "model": "whisper-1"
        # No language specified - OpenAI auto-detects!
    }
    logger.info("[OpenAI Client]    Input transcription: enabled")
```

**After:**
```python
if self.enable_input_transcription:
    session_config["input_audio_transcription"] = {
        "model": "whisper-1",
        "language": "en"  # Force English transcription
    }
    logger.info("[OpenAI Client]    Input transcription: enabled (language: en)")
```

### Why This Works

1. **Whisper Language Constraint** - Setting `"language": "en"` tells Whisper to transcribe audio as English
2. **Response Language Signal** - OpenAI uses transcription language as a strong signal for response language
3. **Overrides Auto-Detection** - Prevents audio-based language detection from overriding system prompt

### Expected Behavior After Fix

1. ‚úÖ User speaks in **any language**
2. ‚úÖ Whisper transcribes as **English** (best effort)
3. ‚úÖ OpenAI responds in **English** (matching system prompt and transcription language)
4. ‚úÖ System prompt "Archie" behavior maintained

## Testing

### Test Plan

1. **Restart backend** - Uvicorn will auto-reload with changes
2. **Start new voice session** - Refresh browser page
3. **Speak test phrases** - Try various accents/languages
4. **Verify responses** - Should all be in English

### Test Commands

```bash
# Monitor logs for language configuration
tail -f /tmp/agentic-logs/backend.log | grep "Input transcription"

# Expected log output:
# [OpenAI Client]    Input transcription: enabled (language: en)
```

### Success Criteria

- [x] Backend reloads without errors
- [ ] Session configuration shows `"language": "en"` in logs
- [ ] Assistant responds in English
- [ ] Assistant follows "Archie" system prompt behavior
- [ ] Transcriptions still work (may be less accurate for non-English speech, but that's acceptable)

## Impact

### User Experience

**Before Fix:**
- User speaks ‚Üí OpenAI detects language ‚Üí Responds in detected language (Spanish/Portuguese/Chinese)
- Confusing for English users
- System prompt behavior ignored

**After Fix:**
- User speaks ‚Üí Whisper transcribes as English ‚Üí OpenAI responds in English
- Consistent English responses
- System prompt "Archie" behavior maintained

### Trade-offs

**Pros:**
- ‚úÖ Consistent English responses
- ‚úÖ System prompt behavior respected
- ‚úÖ Matches expected user experience

**Cons:**
- ‚ö†Ô∏è Non-English speakers must speak English to get accurate transcriptions
- ‚ö†Ô∏è Transcription accuracy may be lower for heavily accented speech
- ‚ö†Ô∏è Cannot use this system for multilingual support without changes

### Multilingual Support (Future)

If multilingual support is needed, we could:

1. **Add language selection UI** - Let user choose language
2. **Pass language parameter** - Add `language` to session configuration
3. **Match system prompt** - Use language-specific system prompts
4. **Auto-detect with confirmation** - Detect first, then confirm with user

## Related Changes

### Previous Fixes in This Session

1. **[WEBRTC_AUDIO_FIX_2025_12_04.md](WEBRTC_AUDIO_FIX_2025_12_04.md)** - Fixed audio flow issue
2. **[VAD_CONFIG_FIX_2025_12_04.md](VAD_CONFIG_FIX_2025_12_04.md)** - Fixed VAD timing

### Files Modified

1. **[backend/api/openai_webrtc_client.py](backend/api/openai_webrtc_client.py)** - Added language constraint to transcription

### Files Analyzed

1. **[backend/api/realtime_voice.py](backend/api/realtime_voice.py)** - Verified system prompt content
2. **[refs/webrtc-bridge/src/openai/openai.realtime.ts](refs/webrtc-bridge/src/openai/openai.realtime.ts)** - Reference implementation (Portuguese)
3. **Conversation exports** - Analyzed transcriptions and responses

## Verification

### Log Confirmation

After backend reload, verify in logs:

```
[OpenAI Client] üîß Configuring session...
[OpenAI Client]    System prompt: You are Archie, the realtime voice interface for a multi‚Äëagent team...
[OpenAI Client]    No tools registered
[OpenAI Client]    VAD: enabled (server_vad with OpenAI defaults)
[OpenAI Client]    Input transcription: enabled (language: en)  ‚Üê THIS LINE IS KEY!
[OpenAI Client] üì§ Sending session.update event
[OpenAI Client] ‚úÖ Session configuration sent
```

### Session Configuration

In conversation exports, verify:

```json
"input_audio_transcription": {
  "model": "whisper-1",
  "language": "en"  // ‚Üê Should be "en" not null!
}
```

## Conclusion

**Problem:** OpenAI was auto-detecting language from audio and responding in Portuguese/Spanish/Chinese

**Root Cause:** No language constraint on Whisper transcription (`"language": null`)

**Solution:** Explicitly set `"language": "en"` in transcription configuration

**Status:** ‚úÖ **FIX IMPLEMENTED** - Awaiting backend reload and testing

**Next Steps:**
1. Wait for uvicorn auto-reload
2. Refresh voice interface
3. Test with English speech
4. Verify English responses
5. Document results

---

**Date:** 2025-12-04
**Author:** Claude (Sonnet 4.5)
**Session:** WebRTC Voice System Debugging
**Related:** WEBRTC_AUDIO_FIX_2025_12_04.md, VAD_CONFIG_FIX_2025_12_04.md
