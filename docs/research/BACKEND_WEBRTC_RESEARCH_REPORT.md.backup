# Backend WebRTC for OpenAI Realtime API: Comprehensive Research Report

**Date:** 2025-12-02
**Objective:** Evaluate feasibility of implementing direct WebRTC connection between Python backend and OpenAI's Realtime API, eliminating frontend dependency for voice connections.

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [Python WebRTC Libraries](#python-webrtc-libraries)
3. [Backend Audio Handling](#backend-audio-handling)
4. [OpenAI Realtime API WebRTC Protocol](#openai-realtime-api-webrtc-protocol)
5. [Architecture Patterns](#architecture-patterns)
6. [Alternative Approaches](#alternative-approaches)
7. [Technical Challenges](#technical-challenges)
8. [Recommended Technical Stack](#recommended-technical-stack)
9. [Implementation Comparison](#implementation-comparison)
10. [Conclusions and Recommendations](#conclusions-and-recommendations)
11. [References](#references)

---

## Executive Summary

**Key Findings:**

1. **Backend WebRTC is feasible** using Python's `aiortc` library (≥1.6.0)
2. **Active development exists** - Multiple projects implementing OpenAI Realtime + aiortc in 2024-2025
3. **LiveKit provides production-ready solution** - Acts as WebRTC-to-WebSocket bridge with edge routing
4. **Audio handling requires careful design** - Multiple libraries available (sounddevice, pyaudio, webrtc-audio-processing)
5. **Latency implications** - Backend proxy adds ~100-200ms vs direct browser WebRTC
6. **Codec compatibility** - aiortc supports Opus (best) and PCM16 (simple), both compatible with OpenAI

**Recommendation:** Consider **hybrid architecture** where backend owns WebRTC connection but leverages existing frontend audio capabilities through data channels or WebSocket streaming. For production, **LiveKit framework** offers mature solution with minimal custom development.

---

## Python WebRTC Libraries

### 1. aiortc - Primary Candidate

**Overview:**
- Pure Python WebRTC implementation built on asyncio
- Most mature and actively maintained Python WebRTC library
- Official PyPI package with regular updates

**GitHub:** [aiortc/aiortc](https://github.com/aiortc/aiortc)
**Documentation:** https://aiortc.readthedocs.io/
**PyPI:** https://pypi.org/project/aiortc/
**Version:** ≥1.6.0 (latest compatible with OpenAI implementations)

**Key Features:**
- Full WebRTC 1.0 and ORTC implementation
- Asyncio-based architecture for non-blocking operations
- SDP offer/answer negotiation support
- Audio/video track management
- Data channels support
- STUN/TURN client capabilities

**Audio Codec Support:**
- Opus 48 kHz (recommended for OpenAI)
- PCM 8 kHz
- PCM 16 kHz

**Example SDP Exchange:**

```python
from aiortc import RTCPeerConnection, RTCSessionDescription

# Create peer connection
pc = RTCPeerConnection()

# Add audio track
audio_track = MicStreamTrack()  # Custom implementation
pc.addTrack(audio_track)

# Create offer
offer = await pc.createOffer()
await pc.setLocalDescription(offer)

# Handle answer from OpenAI
answer = RTCSessionDescription(sdp=answer_sdp, type="answer")
await pc.setRemoteDescription(answer)
```

**Maturity Assessment:**
- ✅ **Stable API** - Consistent over multiple versions
- ✅ **Active maintenance** - Regular bug fixes and updates
- ✅ **Good documentation** - Examples and API reference available
- ⚠️ **Performance** - Pure Python implementation, CPU-intensive for large scale
- ⚠️ **Production usage** - Limited large-scale deployments documented

**Known Limitations:**
- CPU overhead compared to native implementations
- Python GIL constraints for multi-threaded scenarios
- Limited to supported codecs (no H.264 without external dependencies)
- Scaling limitations (~100 concurrent connections per instance)

**Resources:**
- [Python WebRTC basics with aiortc](https://dev.to/whitphx/python-webrtc-basics-with-aiortc-48id)
- [aiortc WebRTC Library Guide](https://webrtc.link/en/articles/aiortc-python-webrtc-library/)
- [Building a Python WebRTC backend with aiortc](https://www.preste.ai/post/building-a-python-webrtc-backend-with-aiortc)

### 2. Alternative Libraries

**pion (Go):**
- Not Python, but frequently mentioned as alternative
- More performant than aiortc
- Used internally by OpenAI's WebRTC proxy

**python-webrtc-audio-processing:**
- Provides WebRTC audio processing algorithms (VAD, noise suppression)
- Bindings to native WebRTC audio processing library
- Complements aiortc rather than replaces it

**GitHub:** [xiongyihui/python-webrtc-audio-processing](https://github.com/xiongyihui/python-webrtc-audio-processing)

---

## Backend Audio Handling

### Audio I/O Libraries

#### 1. sounddevice (Recommended)

**PyPI:** https://pypi.org/project/sounddevice/
**Version:** ≥0.4.6
**Documentation:** https://python-sounddevice.readthedocs.io/

**Advantages:**
- Excellent NumPy integration
- Callback-based streaming
- Low-latency operation
- Cross-platform (PortAudio backend)
- Clean API design

**Example Audio Capture:**

```python
import sounddevice as sd
import numpy as np
from queue import Queue

audio_queue = Queue()

def audio_callback(indata, frames, time, status):
    """Called by sounddevice for each audio block"""
    if status:
        print(f"Audio status: {status}")
    # Convert to int16 PCM
    audio_queue.put(indata.copy())

# Start streaming
stream = sd.InputStream(
    samplerate=16000,
    channels=1,
    dtype='int16',
    callback=audio_callback,
    blocksize=1024
)
stream.start()
```

**Performance:** Can utilize 70%+ of available CPU time in callbacks without glitches.

#### 2. PyAudio

**PyPI:** https://pypi.org/project/PyAudio/
**Version:** ≥0.2.13

**Characteristics:**
- Direct PortAudio bindings
- More low-level than sounddevice
- Wider adoption in legacy projects
- Less ergonomic API

**Use Case:** When you need maximum control over audio parameters.

#### 3. webrtc-audio-processing

**PyPI:** https://pypi.org/project/webrtc-audio-processing/

**Features:**
- Voice Activity Detection (VAD)
- Noise Suppression (NS)
- Echo Cancellation
- Automatic Gain Control

**Integration Example:**

```python
from webrtcvad import Vad

vad = Vad(3)  # Aggressiveness level 0-3

# Check if frame contains speech
is_speech = vad.is_speech(audio_frame, sample_rate=16000)
```

### Audio Format Conversion

**NumPy-based conversion** between formats:

```python
import numpy as np

# Float32 [-1.0, 1.0] to int16 PCM
def float_to_pcm16(audio_float):
    return (audio_float * 32767).astype(np.int16)

# int16 PCM to base64 for OpenAI
import base64
def pcm_to_base64(pcm_data):
    return base64.b64encode(pcm_data.tobytes()).decode('utf-8')
```

### Virtual Audio Devices

**Use Case:** Route audio between applications without physical devices.

#### PulseAudio Virtual Sinks (Linux)

**Create null sink:**
```bash
pacmd load-module module-null-sink sink_name=VirtualOutput
```

**Create virtual microphone:**
```bash
pacmd load-module module-remap-source \
  master=VirtualOutput.monitor \
  source_name=VirtualMic
```

**Python integration:**
```python
import sounddevice as sd

# List available devices
print(sd.query_devices())

# Use virtual device
stream = sd.InputStream(device='VirtualMic')
```

**Resources:**
- [Virtual microphone using GStreamer and PulseAudio](https://aweirdimagination.net/2020/07/19/virtual-microphone-using-gstreamer-and-pulseaudio/)
- [Python Pulseaudio Loopback Tool](https://github.com/alentoghostflame/Python-Pulseaudio-Loopback-Tool)

#### ALSA Loopback (Linux)

**Load loopback module:**
```bash
sudo modprobe snd-aloop
```

**Access from Python:**
```python
# ALSA loopback appears as hw:Loopback,0,0
stream = sd.InputStream(device='hw:Loopback,0,0')
```

### Audio Mixing for Multiple Clients

**Scenario:** Mix audio from desktop + mobile for unified output.

**NumPy mixing approach:**

```python
import numpy as np

def mix_audio_streams(*streams):
    """Mix multiple audio streams by averaging"""
    # Ensure all streams have same length
    min_len = min(len(s) for s in streams)
    truncated = [s[:min_len] for s in streams]

    # Average (simple mixing)
    mixed = np.mean(truncated, axis=0)

    # Prevent clipping
    mixed = np.clip(mixed, -32768, 32767)

    return mixed.astype(np.int16)
```

**Performance:** Mixing 6 streams of 20ms frames takes <20ms with NumPy (real-time capable).

**Advanced mixing with Web Audio API concepts:**

```python
# Per-participant mixing (conference scenario)
def mix_for_participant(participant_id, all_streams):
    """Mix all streams except participant's own voice"""
    other_streams = [
        stream for pid, stream in all_streams.items()
        if pid != participant_id
    ]
    return mix_audio_streams(*other_streams)
```

**Resources:**
- [PyAudio mixing multiple tracks and channels](https://stackoverflow.com/questions/30293137/pyaudio-mixing-multiple-tracks-and-channels)
- [PyAudioMixer](https://github.com/nvahalik/PyAudioMixer)
- [Building a Real-time Collaborative Audio Mixer](https://www.imseankim.com/journal/realtime-collaborative-audio-mixer-web-audio-webrtc)

---

## OpenAI Realtime API WebRTC Protocol

### Architecture Overview

**Official Documentation:** [Realtime API with WebRTC](https://platform.openai.com/docs/guides/realtime-webrtc)

**Key Insight:** OpenAI uses a pion-based server (Go WebRTC implementation) that proxies WebRTC data channel messages to an internal WebSocket server. This means:
- Message format identical to WebSocket API
- WebRTC used only for transport and codec benefits
- Signaling through standard offer/answer exchange

**Internal Architecture:**
```
Client (Browser/Python)
    ↓ WebRTC (Opus audio)
OpenAI pion Proxy
    ↓ WebSocket
OpenAI Realtime Server
```

### Authentication

#### Ephemeral Tokens (Recommended)

**Purpose:** Secure temporary tokens that expire in 60 seconds (2-hour TTL in practice).

**Backend Implementation:**

```python
import aiohttp
import os

async def get_ephemeral_token():
    """Generate ephemeral token from server"""
    api_key = os.getenv('OPENAI_API_KEY')

    url = "https://api.openai.com/v1/realtime/sessions"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": "gpt-4o-realtime-preview-2024-12-17",
        "voice": "alloy"
    }

    async with aiohttp.ClientSession() as session:
        async with session.post(url, json=payload, headers=headers) as resp:
            data = await resp.json()
            return data['client_secret']['value']
```

**Security Benefits:**
- Prevents exposure of long-lived API keys
- Client receives temporary token only
- Server maintains control over access

**Token Exchange Endpoint:**

```python
from fastapi import FastAPI, HTTPException

app = FastAPI()

@app.get("/api/realtime/token")
async def get_token():
    try:
        token = await get_ephemeral_token()
        return {"token": token}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

**Resources:**
- [OpenAI Realtime API Ephemeral Tokens](https://community.openai.com/t/openai-realtime-api-ephemeral-tokens/1082851)
- [Ephemeral Key Server Guide](https://deepwiki.com/amararun/shared-openai-realtime-webrtc-cricket/5.1-ephemeral-key-server)

### SDP Exchange Protocol

**Standard WebRTC signaling:**

1. **Client creates offer**
2. **Client sends offer to backend**
3. **Backend forwards offer to OpenAI** (`POST /realtime`)
4. **OpenAI returns answer**
5. **Backend forwards answer to client**
6. **WebRTC connection established**

**Python Backend Relay:**

```python
async def exchange_sdp_with_openai(local_sdp: str, token: str):
    """Exchange SDP with OpenAI Realtime API"""
    url = "https://api.openai.com/v1/realtime"
    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/sdp"
    }

    async with aiohttp.ClientSession() as session:
        async with session.post(url, data=local_sdp, headers=headers) as resp:
            answer_sdp = await resp.text()
            return answer_sdp
```

### Audio Codec Requirements

**OpenAI Realtime API supports:**

| Codec | Sample Rate | Use Case | Bandwidth | Latency |
|-------|-------------|----------|-----------|---------|
| Opus | 48 kHz | **Recommended** | ~65% less than PCM | Lowest (~100-150ms faster) |
| PCM16 | 16 kHz | Simple, no decoding | Higher | Moderate |
| PCM16 | 24 kHz | High quality | Highest | Moderate |
| G.711 μ-law | 8 kHz | Legacy telephony | Low (compressed) | Moderate |

**Recommendation:** Use **Opus** for both input and output when possible.

**Format Configuration:**

```python
# In session configuration
session_config = {
    "model": "gpt-4o-realtime-preview-2024-12-17",
    "modalities": ["text", "audio"],
    "instructions": "You are a helpful assistant.",
    "voice": "alloy",
    "input_audio_format": "pcm16",    # or "opus"
    "output_audio_format": "opus",    # Recommended
    "input_audio_transcription": {
        "model": "whisper-1"
    },
    "turn_detection": {
        "type": "server_vad",
        "threshold": 0.5,
        "prefix_padding_ms": 300,
        "silence_duration_ms": 500
    }
}
```

**aiortc Codec Mapping:**

```python
from aiortc import RTCPeerConnection

# aiortc automatically negotiates Opus for audio
# No manual codec configuration needed
pc = RTCPeerConnection()

# Audio track will use Opus if both peers support it
audio_track = AudioStreamTrack()
pc.addTrack(audio_track)
```

**Important:** aiortc only supports Opus, PCM 8kHz, and PCM 16kHz for incoming tracks.

### Message Protocol

**Event-driven API over data channel:**

**Input Events (Client → OpenAI):**

```python
# Append audio to buffer
{
    "type": "input_audio_buffer.append",
    "audio": "base64_encoded_audio"
}

# Commit buffer and request response
{
    "type": "input_audio_buffer.commit"
}

# Generate response
{
    "type": "response.create",
    "response": {
        "modalities": ["audio", "text"],
        "instructions": "Please assist the user."
    }
}
```

**Output Events (OpenAI → Client):**

```python
# Audio delta (streaming)
{
    "type": "response.audio.delta",
    "response_id": "resp_001",
    "item_id": "item_001",
    "output_index": 0,
    "content_index": 0,
    "delta": "base64_encoded_audio"
}

# Transcription
{
    "type": "conversation.item.input_audio_transcription.completed",
    "transcript": "Hello, how can I help you?"
}

# Response completion
{
    "type": "response.done",
    "response": { ... }
}
```

**Resources:**
- [The Unofficial Guide to OpenAI's Realtime WebRTC API](https://webrtchacks.com/the-unofficial-guide-to-openai-realtime-webrtc-api/)
- [OpenAI Realtime API Documentation](https://platform.openai.com/docs/guides/realtime)

---

## Architecture Patterns

### Pattern 1: Direct Backend WebRTC (Full Ownership)

**Diagram:**

```
┌─────────────────┐
│  Frontend       │
│  (UI Only)      │
└────────┬────────┘
         │ WebSocket (control)
         │
┌────────▼────────┐      WebRTC (audio)      ┌──────────────┐
│  Python         ├─────────────────────────►│   OpenAI     │
│  Backend        │◄─────────────────────────┤   Realtime   │
│                 │                           │   API        │
│  - aiortc       │                           └──────────────┘
│  - sounddevice  │
│  - audio mix    │
└─────────────────┘
         ▲
         │ Audio I/O
    ┌────┴────┐
    │ Speaker │
    │   Mic   │
    └─────────┘
```

**Implementation:**

```python
import asyncio
from aiortc import RTCPeerConnection, RTCSessionDescription
import sounddevice as sd
import numpy as np
from queue import Queue

class BackendWebRTCSession:
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.pc = RTCPeerConnection()
        self.audio_queue = Queue()
        self.is_running = False

    async def connect_to_openai(self):
        """Establish WebRTC connection with OpenAI"""
        # Get ephemeral token
        token = await self.get_ephemeral_token()

        # Add local audio track
        audio_track = MicrophoneTrack(self.audio_queue)
        self.pc.addTrack(audio_track)

        # Handle incoming audio
        @self.pc.on("track")
        async def on_track(track):
            if track.kind == "audio":
                await self.handle_audio_stream(track)

        # Create offer
        offer = await self.pc.createOffer()
        await self.pc.setLocalDescription(offer)

        # Exchange SDP
        answer_sdp = await self.exchange_sdp(offer.sdp, token)
        answer = RTCSessionDescription(sdp=answer_sdp, type="answer")
        await self.pc.setRemoteDescription(answer)

        # Wait for connection
        await self.wait_for_connection()

    def start_audio_capture(self):
        """Capture audio from microphone"""
        def callback(indata, frames, time, status):
            self.audio_queue.put(indata.copy())

        self.stream = sd.InputStream(
            samplerate=16000,
            channels=1,
            dtype='int16',
            callback=callback
        )
        self.stream.start()
        self.is_running = True

    async def handle_audio_stream(self, track):
        """Play received audio"""
        while self.is_running:
            frame = await track.recv()
            # Convert to numpy and play
            audio_data = np.frombuffer(frame.data, dtype=np.int16)
            sd.play(audio_data, samplerate=16000)
            await asyncio.sleep(0.01)
```

**Pros:**
- ✅ Full control over audio pipeline
- ✅ Can implement custom audio processing
- ✅ No frontend audio complexity
- ✅ Backend-owned session management
- ✅ Easier server-side audio recording/monitoring

**Cons:**
- ❌ Backend server requires audio devices (virtual or physical)
- ❌ Adds ~100-200ms latency vs direct browser connection
- ❌ Complicates deployment (audio device configuration)
- ❌ Difficult to provide UI audio controls (mic selection, etc.)
- ❌ Doesn't leverage browser's echo cancellation/noise suppression
- ❌ Not suitable for multi-user scenarios (each needs separate server audio stream)

**Best For:**
- Server-side automation (no human user)
- Single-user desktop applications
- Scenarios where audio processing is needed server-side

### Pattern 2: Backend Proxy (Signaling Only)

**Diagram:**

```
┌─────────────────┐      WebRTC (audio)      ┌──────────────┐
│  Frontend       ├─────────────────────────►│   OpenAI     │
│  (Browser)      │◄─────────────────────────┤   Realtime   │
│                 │                           │   API        │
│  - WebRTC       │                           └──────────────┘
│  - Web Audio    │              ▲
└────────┬────────┘              │
         │                       │ SDP exchange
         │ WebSocket             │ (signaling)
         │                       │
┌────────▼───────────────────────┘
│  Python Backend                │
│                                │
│  - Token generation            │
│  - SDP relay                   │
│  - Event forwarding            │
└────────────────────────────────┘
```

**Implementation:**

```python
from fastapi import FastAPI, WebSocket
import aiohttp

app = FastAPI()

@app.websocket("/ws/realtime")
async def realtime_proxy(websocket: WebSocket):
    await websocket.accept()

    # Get ephemeral token
    token = await get_ephemeral_token()
    await websocket.send_json({"type": "token", "token": token})

    # Relay SDP exchange
    offer_data = await websocket.receive_json()
    offer_sdp = offer_data['sdp']

    # Forward to OpenAI
    answer_sdp = await exchange_sdp_with_openai(offer_sdp, token)
    await websocket.send_json({"type": "answer", "sdp": answer_sdp})

    # Connection established, frontend handles audio
```

**Pros:**
- ✅ Leverages browser's native WebRTC (lowest latency)
- ✅ Browser handles echo cancellation, noise suppression
- ✅ User can select audio devices
- ✅ Backend doesn't need audio hardware
- ✅ Scalable (no server-side audio processing)

**Cons:**
- ❌ Backend has limited visibility into audio stream
- ❌ Harder to implement server-side recording
- ❌ Requires frontend WebRTC implementation

**Best For:**
- Multi-user web applications
- Production deployments
- When low latency is critical

**Current Implementation:** This is effectively what your existing mobile voice system uses.

### Pattern 3: Hybrid (Backend Controls, Frontend Audio)

**Diagram:**

```
┌─────────────────┐
│  Frontend       │      Audio via WebSocket
│  (Browser)      ├──────────────┐
│                 │              │
│  - Mic capture  │              │
│  - Speaker out  │              ▼
└────────┬────────┘      ┌───────────────┐      WebRTC      ┌──────────────┐
         │                │  Python       ├─────────────────►│   OpenAI     │
         │ WebSocket      │  Backend      │◄─────────────────┤   Realtime   │
         │ (control)      │               │                  │   API        │
         └───────────────►│  - aiortc     │                  └──────────────┘
                          │  - audio mix  │
                          │  - routing    │
                          └───────────────┘
```

**Implementation:**

```python
class HybridWebRTCSession:
    def __init__(self):
        self.pc = RTCPeerConnection()
        self.client_connections = {}  # WebSocket connections

    async def add_client_audio_source(self, client_id: str, websocket: WebSocket):
        """Receive audio from frontend via WebSocket"""
        self.client_connections[client_id] = websocket

        try:
            while True:
                # Receive audio chunks from frontend
                audio_data = await websocket.receive_bytes()

                # Forward to OpenAI via WebRTC data channel
                await self.send_audio_to_openai(audio_data)
        except WebSocketDisconnect:
            del self.client_connections[client_id]

    async def handle_openai_audio(self, track):
        """Receive audio from OpenAI, broadcast to clients"""
        while True:
            frame = await track.recv()
            audio_bytes = frame.data

            # Send to all connected clients
            for ws in self.client_connections.values():
                await ws.send_bytes(audio_bytes)
```

**Pros:**
- ✅ Backend controls OpenAI connection
- ✅ Frontend handles audio I/O (devices, echo cancellation)
- ✅ Server can process/record audio stream
- ✅ Flexible routing (multiple clients)

**Cons:**
- ⚠️ More complex architecture
- ⚠️ WebSocket audio streaming (not WebRTC efficiency)
- ⚠️ Requires both WebRTC and WebSocket

**Best For:**
- Advanced use cases needing backend audio processing
- Multi-device scenarios (desktop + mobile)

### Pattern 4: LiveKit Framework (Production-Ready)

**Diagram:**

```
┌─────────────────┐      WebRTC (audio)      ┌──────────────┐
│  Frontend       ├─────────────────────────►│   LiveKit    │
│  (Browser)      │◄─────────────────────────┤   SFU        │
│                 │                           │              │
│  - LiveKit SDK  │                           └──────┬───────┘
└─────────────────┘                                  │
                                                     │ Low-latency
                                                     │ proprietary
                                                     │ protocol
                                            ┌────────▼────────┐      WebSocket      ┌──────────────┐
                                            │  Python Agent   ├───────────────────►│   OpenAI     │
                                            │  (LiveKit       │◄───────────────────┤   Realtime   │
                                            │   Agents SDK)   │                    │   API        │
                                            │                 │                    └──────────────┘
                                            │  - Audio relay  │
                                            │  - Tool calling │
                                            │  - State mgmt   │
                                            └─────────────────┘
```

**Implementation:**

```python
from livekit.agents import AutoSubscribe, JobContext, WorkerOptions, cli
from livekit.agents.multimodal import MultimodalAgent
from livekit.plugins import openai

async def entrypoint(ctx: JobContext):
    """LiveKit agent entry point"""

    # Connect to room
    await ctx.connect(auto_subscribe=AutoSubscribe.AUDIO_ONLY)

    # Create OpenAI Realtime agent
    agent = MultimodalAgent(
        model="gpt-4o-realtime-preview-2024-12-17",
        voice="alloy",
        turn_detection={
            "type": "server_vad",
            "threshold": 0.5
        }
    )

    # Start agent session
    agent.start(ctx.room)

    # Handle function calls
    @agent.on("function_call")
    async def on_function_call(call):
        if call.name == "get_weather":
            return {"temp": 72, "condition": "sunny"}

if __name__ == "__main__":
    cli.run_app(WorkerOptions(entrypoint_fnc=entrypoint))
```

**Pros:**
- ✅ Production-ready, battle-tested
- ✅ Edge routing for minimal latency
- ✅ Built-in audio mixing (SFU)
- ✅ Scalable to 1000+ participants
- ✅ Automatic reconnection handling
- ✅ Monitoring and analytics
- ✅ Official OpenAI partnership

**Cons:**
- ❌ External dependency (LiveKit server)
- ❌ More complex setup
- ❌ Potential cost (cloud-hosted)
- ❌ Less control over internals

**Best For:**
- **Production deployments**
- Multi-user voice applications
- When reliability and scale are critical

**Resources:**
- [LiveKit Agents Documentation](https://docs.livekit.io/agents/)
- [OpenAI and LiveKit Integration](https://docs.livekit.io/agents/integrations/openai/realtime/)
- [LiveKit GitHub](https://github.com/livekit/agents)

---

## Alternative Approaches

### Approach 1: WebSocket-Only (No WebRTC)

**When:** Backend automation, CLI tools, batch processing

**Advantages:**
- ✅ Simpler implementation
- ✅ No WebRTC complexity
- ✅ Better for scripted scenarios

**Limitations:**
- ❌ No Opus codec benefits (PCM16 only)
- ❌ Higher bandwidth usage
- ❌ Slightly higher latency
- ❌ No browser echo cancellation

**Python Example:**

```python
import websockets
import asyncio
import base64
import json
import sounddevice as sd
import numpy as np

async def websocket_realtime():
    url = "wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-12-17"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "OpenAI-Beta": "realtime=v1"
    }

    async with websockets.connect(url, extra_headers=headers) as ws:
        # Configure session
        await ws.send(json.dumps({
            "type": "session.update",
            "session": {
                "modalities": ["audio", "text"],
                "voice": "alloy",
                "input_audio_format": "pcm16",
                "output_audio_format": "pcm16"
            }
        }))

        # Capture and send audio
        async def send_audio():
            def callback(indata, frames, time, status):
                pcm16 = (indata * 32767).astype(np.int16)
                b64 = base64.b64encode(pcm16.tobytes()).decode()
                asyncio.create_task(ws.send(json.dumps({
                    "type": "input_audio_buffer.append",
                    "audio": b64
                })))

            stream = sd.InputStream(callback=callback, channels=1, samplerate=24000)
            with stream:
                await asyncio.sleep(10)  # Record for 10 seconds

        # Receive and play audio
        async def receive_audio():
            async for message in ws:
                data = json.loads(message)
                if data['type'] == 'response.audio.delta':
                    audio_b64 = data['delta']
                    audio_bytes = base64.b64decode(audio_b64)
                    audio_pcm = np.frombuffer(audio_bytes, dtype=np.int16)
                    sd.play(audio_pcm, samplerate=24000)

        await asyncio.gather(send_audio(), receive_audio())
```

**Best For:** Python CLI tools, server-side automation, batch voice tasks.

**Resources:**
- [OpenAI Realtime API Python Guide 2025](https://skywork.ai/blog/agent/openai-realtime-api-python-guide-2025-complete-code/)

### Approach 2: Backend SFU (Selective Forwarding Unit)

**When:** Multi-party voice calls, conferencing

**Architecture Types:**

| Type | Description | Use Case | Complexity |
|------|-------------|----------|------------|
| **Mesh** | Peer-to-peer, everyone connects to everyone | Small groups (<5) | Low |
| **MCU** | Server mixes all audio into single stream | Medium groups (5-30) | High |
| **SFU** | Server forwards streams without mixing | Large groups (30+) | Medium |

**Python SFU Options:**

**None production-ready** - SFU servers are typically C++/Go:
- Janus (C)
- Mediasoup (Node.js/Rust)
- Kurento (C++, deprecated)

**Recommendation:** Use LiveKit (includes SFU) or interface with existing SFU via API.

**SFU Audio Flow:**

```
Client A ──► SFU ──► Client B
                └──► Client C
Client B ──► SFU ──► Client A
                └──► Client C
```

Each client sends one stream, receives N-1 streams.

**Resources:**
- [WebRTC conferences - to mix or to route audio](https://bloggeek.me/webrtc-conferences-mix-or-route-audio/)
- [Comparative Study of WebRTC SFUs](https://mediasoup.org/resources/CoSMo_ComparativeStudyOfWebrtcOpenSourceSfusForVideoConferencing.pdf)

### Approach 3: TTS + STT (Text Pipeline)

**When:** Text-based agents with voice interface

**Flow:**

```
User Speech → STT → Text → Agent Logic → Text → TTS → Audio Output
```

**Advantages:**
- ✅ Simpler than WebRTC
- ✅ Can use any TTS/STT provider
- ✅ Easier to debug (text intermediate)

**Disadvantages:**
- ❌ Higher latency (sequential processing)
- ❌ Loses voice nuances
- ❌ Not "real-time" conversation

**Example:**

```python
from openai import AsyncOpenAI

client = AsyncOpenAI()

# STT
transcription = await client.audio.transcriptions.create(
    model="whisper-1",
    file=audio_file
)

# Agent processing
response = await client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": transcription.text}]
)

# TTS
speech = await client.audio.speech.create(
    model="tts-1",
    voice="alloy",
    input=response.choices[0].message.content
)
```

**Best For:** When latency >2 seconds is acceptable, need text-based logging.

---

## Technical Challenges

### Challenge 1: Latency

**Sources of Latency:**

| Component | Latency | Notes |
|-----------|---------|-------|
| Audio capture | 20-50ms | Depends on buffer size |
| Network (WebRTC) | 50-100ms | STUN/TURN can add 50-150ms |
| Backend proxy | 100-200ms | If routing through server |
| OpenAI processing | 200-500ms | Model inference time |
| Audio playback | 20-50ms | Output buffer |
| **Total (direct)** | **290-700ms** | Browser → OpenAI direct |
| **Total (proxy)** | **390-900ms** | Browser → Backend → OpenAI |

**Mitigation Strategies:**

1. **Use Opus codec** - Saves ~100-150ms vs PCM
2. **Minimize buffer sizes** - Balance latency vs glitches
3. **Direct peer connections** - Avoid TURN when possible
4. **Edge routing** - LiveKit uses global edge network
5. **Optimize WebSocket frames** - Send smaller audio chunks

**Benchmark Data:**
- Direct WebRTC: 90% of connections <100ms RTT
- With TURN relay: +50-150ms
- Backend proxy: +100-200ms

**Resources:**
- [Understanding WebRTC Latency](https://www.videosdk.live/developer-hub/webrtc/webrtc-latency)
- [WebRTC lowest possible latency](https://stackoverflow.com/questions/42194470/webrtc-lowest-possible-latency)

### Challenge 2: Audio Quality

**Issues:**

1. **Echo** - User hears their own voice back
   - **Solution:** Use browser echo cancellation (AEC)
   - **Backend approach:** webrtc-audio-processing library

2. **Noise** - Background noise interferes
   - **Solution:** Noise suppression (NS)
   - **Backend approach:** webrtc-audio-processing library

3. **Clipping** - Audio peaks distort
   - **Solution:** Automatic Gain Control (AGC)

**Echo Cancellation Example:**

```python
from webrtcvad import Vad
import audioop

vad = Vad(3)  # Aggressive VAD

def process_audio_frame(frame, sample_rate=16000):
    # Voice activity detection
    is_speech = vad.is_speech(frame, sample_rate)

    if not is_speech:
        # Suppress non-speech frames
        return b'\x00' * len(frame)

    # Apply gain control
    frame = audioop.mul(frame, 2, 1.5)  # Amplify

    return frame
```

**Codec Quality Comparison:**

| Codec | Bitrate | Quality | Use Case |
|-------|---------|---------|----------|
| Opus | 6-510 kbps | Excellent | **Recommended** |
| PCM16 | 256 kbps | Perfect (uncompressed) | High bandwidth |
| G.711 | 64 kbps | Good (telephony) | Legacy systems |

### Challenge 3: Multi-Client Audio Mixing

**Scenario:** Desktop + mobile both capturing audio, need unified stream to OpenAI.

**Options:**

**Option A: Server-Side Mixing**

```python
import numpy as np

def mix_stereo_to_mono(*streams):
    """Mix multiple mono streams"""
    # Normalize
    normalized = [s / np.max(np.abs(s)) for s in streams]

    # Average (prevents clipping)
    mixed = np.mean(normalized, axis=0)

    # Scale back to int16
    return (mixed * 32767).astype(np.int16)
```

**Performance:** Mixing 6 streams @ 20ms chunks = <20ms processing (real-time capable).

**Option B: Client-Side Mixing (Web Audio API)**

```javascript
// Browser handles mixing before sending to backend
const context = new AudioContext();
const mixer = context.createChannelMerger(2);

micStream.connect(mixer, 0, 0);  // Desktop mic
mobileStream.connect(mixer, 0, 1);  // Mobile mic

mixer.connect(context.destination);
```

**Resources:**
- [Building a Real-time Collaborative Audio Mixer](https://www.imseankim.com/journal/realtime-collaborative-audio-mixer-web-audio-webrtc)
- [Your Browser as Audio Conference Server](https://webrtchacks.com/web-audio-conference/)

### Challenge 4: Error Recovery

**WebRTC Connection Failures:**

```python
@pc.on("connectionstatechange")
async def on_connection_state_change():
    state = pc.connectionState

    if state == "failed":
        # Attempt reconnection
        await reconnect()
    elif state == "disconnected":
        # Wait before reconnecting
        await asyncio.sleep(2)
        await reconnect()
    elif state == "connected":
        logger.info("Connection established")
```

**Audio Stream Interruption:**

```python
class RobustAudioTrack(MediaStreamTrack):
    kind = "audio"

    def __init__(self):
        super().__init__()
        self.buffer = []
        self.is_recovering = False

    async def recv(self):
        try:
            frame = await self.source.recv()
            self.buffer.append(frame)
            return frame
        except Exception as e:
            # Use buffered frame during recovery
            if self.buffer:
                return self.buffer[-1]
            raise
```

### Challenge 5: Scalability

**aiortc Limitations:**

- ~50-100 concurrent connections per instance
- CPU-bound (pure Python)
- GIL constraints for threading

**Solutions:**

1. **Horizontal scaling** - Multiple backend instances
2. **Use LiveKit** - Production-grade SFU
3. **Optimize audio processing** - Use C extensions (NumPy)
4. **Offload to GPU** - For ML-based audio processing

**Load Balancing:**

```python
# Multiple backend instances behind nginx
upstream webrtc_backends {
    least_conn;
    server backend1:8000;
    server backend2:8000;
    server backend3:8000;
}
```

### Challenge 6: Deployment Complexity

**Backend Audio Devices:**

**Problem:** Server needs audio I/O devices for aiortc.

**Solutions:**

1. **Virtual devices (PulseAudio)**
   ```bash
   pacmd load-module module-null-sink sink_name=virtual_speaker
   pacmd load-module module-null-source source_name=virtual_mic
   ```

2. **ALSA loopback**
   ```bash
   sudo modprobe snd-aloop
   ```

3. **Dummy driver (headless)**
   ```bash
   sudo modprobe snd-dummy
   ```

**Docker considerations:**

```dockerfile
FROM python:3.11

# Install audio dependencies
RUN apt-get update && apt-get install -y \
    pulseaudio \
    alsa-utils \
    libportaudio2 \
    && rm -rf /var/lib/apt/lists/*

# Configure dummy audio
RUN echo "load-module module-null-sink sink_name=virtual_speaker" >> /etc/pulse/default.pa
```

---

## Recommended Technical Stack

### Option A: Pure Backend WebRTC (Full Control)

**Use Case:** Desktop application, single-user, need audio processing

**Stack:**
```
Python 3.11+
├── aiortc==1.6.0              # WebRTC implementation
├── sounddevice==0.4.6         # Audio I/O
├── numpy==1.24.0              # Audio processing
├── webrtc-audio-processing    # VAD, NS, AEC
├── aiohttp==3.9.0             # HTTP client
└── python-dotenv              # Configuration
```

**Architecture:** Pattern 1 (Direct Backend WebRTC)

**Estimated Implementation Time:** 2-3 weeks

**Pros:**
- Full control
- Custom audio processing
- Backend-owned session

**Cons:**
- Requires audio device setup
- Higher latency
- Complex deployment

### Option B: Backend Proxy (Recommended for Web Apps)

**Use Case:** Multi-user web application, production deployment

**Stack:**
```
Python 3.11+ (Backend)
├── fastapi==0.104.0           # API framework
├── aiohttp==3.9.0             # HTTP client
└── python-dotenv              # Configuration

JavaScript/TypeScript (Frontend)
├── @openai/realtime-api       # Official SDK
├── react==18                  # UI framework
└── webrtc-adapter             # WebRTC polyfills
```

**Architecture:** Pattern 2 (Backend Proxy)

**Estimated Implementation Time:** 1 week

**Pros:**
- Leverages browser WebRTC
- Low latency
- Scalable
- Simpler deployment

**Cons:**
- Limited audio control from backend
- Requires frontend implementation

**Status:** This is essentially your current mobile voice architecture.

### Option C: LiveKit Framework (Recommended for Production)

**Use Case:** Production deployment, multi-user, need reliability and scale

**Stack:**
```
Python 3.11+
├── livekit==0.16.0                              # LiveKit SDK
├── livekit-agents==1.0.0                        # Agent framework
├── livekit-plugins-openai==0.8.0                # OpenAI integration
└── livekit-plugins-silero==0.6.4                # VAD

Infrastructure
├── livekit-server (Docker/Cloud)                # SFU server
└── Redis (optional, for state)                  # State management
```

**Architecture:** Pattern 4 (LiveKit Framework)

**Estimated Implementation Time:** 1-2 weeks (including setup)

**Pros:**
- Production-ready
- Built-in SFU (multi-party)
- Edge routing
- Monitoring/analytics
- Official OpenAI partnership

**Cons:**
- External dependency
- Setup complexity
- Potential hosting cost

**Cost Estimate:**
- Self-hosted: Free (Docker)
- LiveKit Cloud: $0.50-$2.00 per participant hour

### Option D: Hybrid Approach

**Use Case:** Need backend control + frontend audio quality

**Stack:**
```
Python 3.11+ (Backend)
├── aiortc==1.6.0              # WebRTC for OpenAI
├── fastapi==0.104.0           # WebSocket server
├── numpy==1.24.0              # Audio mixing
└── aiohttp==3.9.0             # HTTP client

JavaScript (Frontend)
├── Web Audio API              # Audio capture/playback
└── WebSocket                  # Audio streaming
```

**Architecture:** Pattern 3 (Hybrid)

**Estimated Implementation Time:** 3-4 weeks

**Pros:**
- Backend controls OpenAI connection
- Frontend handles audio I/O
- Flexible routing

**Cons:**
- Most complex
- Requires both WebRTC and WebSocket
- Custom protocol needed

---

## Implementation Comparison

### Latency Comparison

| Approach | Total Latency | Components |
|----------|---------------|------------|
| **Current (Browser WebRTC)** | 290-700ms | Capture (30ms) + Network (50ms) + OpenAI (200ms) + Playback (30ms) |
| **Backend WebRTC (Full)** | 390-900ms | +100ms backend routing |
| **Backend Proxy** | 290-700ms | Same as browser (backend only relays SDP) |
| **LiveKit** | 250-600ms | -50ms edge routing optimization |
| **WebSocket-Only** | 390-900ms | No Opus benefit, higher bandwidth |
| **Hybrid** | 340-800ms | +50ms WebSocket audio streaming |

### Complexity Comparison

| Approach | Backend | Frontend | Deployment | Scalability | Maintainability |
|----------|---------|----------|------------|-------------|-----------------|
| **Current** | ⭐ | ⭐⭐⭐ | ⭐ | ⭐⭐⭐ | ⭐⭐ |
| **Backend WebRTC** | ⭐⭐⭐⭐ | ⭐ | ⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐ |
| **Backend Proxy** | ⭐⭐ | ⭐⭐⭐ | ⭐ | ⭐⭐⭐⭐ | ⭐⭐ |
| **LiveKit** | ⭐⭐ | ⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ |
| **Hybrid** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ |

⭐ = Low, ⭐⭐⭐⭐⭐ = High

### Feature Comparison

| Feature | Backend WebRTC | Backend Proxy | LiveKit | Hybrid |
|---------|---------------|---------------|---------|--------|
| **Backend audio control** | ✅ Full | ❌ None | ⚠️ Limited | ✅ Full |
| **Low latency** | ⚠️ Moderate | ✅ Best | ✅ Excellent | ⚠️ Moderate |
| **Multi-user** | ⚠️ Complex | ✅ Easy | ✅ Built-in | ✅ Possible |
| **Audio processing** | ✅ Full | ❌ None | ⚠️ Via plugins | ✅ Full |
| **Deployment** | ⚠️ Complex | ✅ Simple | ⚠️ Moderate | ⚠️ Complex |
| **Cost** | Low | Low | $$$ | Low |
| **Scalability** | ⚠️ Limited | ✅ High | ✅ Very high | ⚠️ Moderate |

### Use Case Recommendations

| Use Case | Best Approach | Reason |
|----------|---------------|--------|
| **Production web app** | LiveKit or Backend Proxy | Low latency, scalable, reliable |
| **Single-user desktop** | Backend WebRTC | Full control, no frontend complexity |
| **Multi-device (desktop+mobile)** | Hybrid or LiveKit | Needs audio mixing, flexible routing |
| **Server automation** | WebSocket-Only | Simplest, no user interaction needed |
| **Prototype/MVP** | Backend Proxy | Fastest implementation, leverages existing frontend |
| **Voice conferencing** | LiveKit | Built-in SFU, multi-party optimized |

---

## Conclusions and Recommendations

### For Your Current System (Agentic Voice Assistant)

**Current Architecture:**
- Frontend: Browser WebRTC → OpenAI Realtime API
- Mobile: Smartphone WebRTC → OpenAI Realtime API
- Backend: WebSocket for control, tool calling, event management

**Recommendation: Backend Proxy Enhancement (Low-Effort Improvement)**

**Rationale:**
1. Your current frontend WebRTC approach already works well
2. Browser provides best latency and audio quality
3. Backend already handles complex logic (nested agents, Claude Code integration)

**Proposed Enhancement:**

```python
# backend/api/realtime_voice.py

class EnhancedRealtimeVoiceController:
    """Enhanced controller with backend audio visibility"""

    def __init__(self):
        self.client_connections = {}
        self.audio_monitor = AudioMonitor()  # New component

    async def handle_client_connection(self, websocket: WebSocket):
        """Enhanced connection with audio monitoring"""
        await websocket.accept()

        # Existing: Token generation
        token = await self.get_ephemeral_token()
        await websocket.send_json({"type": "token", "token": token})

        # New: Optional audio mirroring
        @websocket.on("audio_frame")
        async def on_audio_frame(data):
            # Mirror audio to backend for monitoring/recording
            await self.audio_monitor.process_frame(data)

        # Existing: Event management continues as-is
        ...
```

**Benefits:**
- ✅ Minimal changes to working system
- ✅ Backend gains audio visibility without taking over WebRTC
- ✅ Can record/analyze audio server-side
- ✅ No latency impact
- ✅ Simple deployment

**Implementation Effort:** 1-2 days

### For Future Enhancements

**Phase 1: Audio Monitoring (Immediate)**

Add backend audio visibility without disrupting current architecture:
- Audio frame mirroring via WebSocket
- Server-side recording for conversation history
- Audio analytics (speech duration, silence detection)

**Phase 2: Multi-Client Mixing (Medium-term)**

If you need multiple simultaneous audio sources:
- Implement server-side audio mixing
- Support desktop + mobile + additional devices
- Use NumPy for efficient mixing

**Phase 3: LiveKit Migration (Long-term)**

Consider migrating to LiveKit when:
- User base grows significantly (>100 concurrent users)
- Need multi-party voice (conferencing)
- Want to offload infrastructure management
- Need guaranteed SLAs

### Alternative: Backend WebRTC for Specific Use Cases

**Consider backend WebRTC if you need:**

1. **Server-side voice automation** (no human user)
   - Automated phone systems
   - Voice-based data entry
   - Batch voice processing

2. **Specialized audio processing**
   - Real-time transcription analysis
   - Sentiment analysis on audio
   - Voice authentication

3. **Jetson Nano deployment** (your production environment)
   - Run voice assistant completely server-side
   - Use virtual audio devices (PulseAudio)
   - Control via web UI (no audio through browser)

**Example Use Case for Your System:**

```python
# "Headless" voice assistant on Jetson Nano
# Listens to physical microphone, speaks through physical speaker
# Controlled via web UI (text only)

class JetsonVoiceAssistant:
    """Voice assistant running on Jetson with physical audio I/O"""

    def __init__(self):
        self.pc = RTCPeerConnection()
        self.openai_connected = False

        # Use Jetson's actual audio devices
        self.mic_device = "hw:tegra-snd-t210ref-mobile-rt565x"
        self.speaker_device = "hw:tegra-snd-t210ref-mobile-rt565x"

    async def start(self):
        """Start voice assistant with physical audio"""
        # Connect to OpenAI via aiortc
        await self.connect_to_openai()

        # Start audio capture from Jetson mic
        self.audio_stream = sd.InputStream(
            device=self.mic_device,
            callback=self.send_to_openai
        )

        # Play responses through Jetson speaker
        @self.pc.on("track")
        async def on_track(track):
            await self.play_through_speaker(track)

        self.audio_stream.start()
```

This would allow using the Jetson as a standalone voice assistant accessible throughout the home, with web UI for monitoring only.

### Technical Decision Matrix

**Choose Backend WebRTC if:**
- ❌ Don't need multi-user web app
- ✅ Need server-side audio processing
- ✅ Single-user or server-side automation
- ✅ Physical audio devices available (Jetson, server)
- ❌ Latency >500ms acceptable

**Choose Backend Proxy (Current Approach) if:**
- ✅ Multi-user web application
- ✅ Lowest latency required
- ✅ Leveraging browser capabilities
- ✅ Simple deployment preferred
- ✅ Scalability important

**Choose LiveKit if:**
- ✅ Production deployment
- ✅ Multi-party voice
- ✅ Need SLA guarantees
- ✅ Budget for infrastructure
- ✅ Want managed solution

**Choose Hybrid if:**
- ✅ Need backend audio control
- ✅ Want frontend audio quality
- ✅ Complex routing required
- ❌ Have development resources
- ❌ Ok with custom protocol

---

## References

### Documentation

- [OpenAI Realtime API with WebRTC](https://platform.openai.com/docs/guides/realtime-webrtc)
- [aiortc Documentation](https://aiortc.readthedocs.io/)
- [LiveKit Agents Documentation](https://docs.livekit.io/agents/)
- [OpenAI and LiveKit Integration](https://docs.livekit.io/agents/integrations/openai/realtime/)
- [sounddevice Documentation](https://python-sounddevice.readthedocs.io/)

### GitHub Repositories

- [aiortc/aiortc](https://github.com/aiortc/aiortc) - WebRTC for Python
- [realtime-ai/openai-realtime-webrtc-python](https://github.com/realtime-ai/openai-realtime-webrtc-python) - OpenAI Realtime Python client
- [livekit/agents](https://github.com/livekit/agents) - LiveKit Agents framework
- [xiongyihui/python-webrtc-audio-processing](https://github.com/xiongyihui/python-webrtc-audio-processing) - Audio processing bindings

### Technical Articles

- [The Unofficial Guide to OpenAI's Realtime WebRTC API](https://webrtchacks.com/the-unofficial-guide-to-openai-realtime-webrtc-api/)
- [OpenAI's Realtime API with WebRTC (Rohan Paul)](https://www.rohan-paul.com/p/openais-realtime-api-with-webrtc)
- [Python WebRTC basics with aiortc](https://dev.to/whitphx/python-webrtc-basics-with-aiortc-48id)
- [Building a Python WebRTC backend with aiortc](https://www.preste.ai/post/building-a-python-webrtc-backend-with-aiortc)
- [WebRTC conferences - to mix or to route audio](https://bloggeek.me/webrtc-conferences-mix-or-route-audio/)
- [Understanding WebRTC Latency](https://www.videosdk.live/developer-hub/webrtc/webrtc-latency)

### Community Resources

- [aiortc GitHub Discussions](https://github.com/aiortc/aiortc/discussions)
- [OpenAI Developer Community - Realtime API](https://community.openai.com/c/api/realtime-api)
- [webrtcHacks Blog](https://webrtchacks.com/)

### Stack Overflow Discussions

- [Real-time Audio Streaming with aiortc](https://stackoverflow.com/questions/79105261/real-time-audio-streaming-issue-in-webrtc-using-python-aiortc)
- [PyAudio mixing multiple tracks](https://stackoverflow.com/questions/30293137/pyaudio-mixing-multiple-tracks-and-channels)
- [WebRTC lowest possible latency](https://stackoverflow.com/questions/42194470/webrtc-lowest-possible-latency)

### Academic Papers

- [Comparative Study of WebRTC Open Source SFUs for Video Conferencing](https://mediasoup.org/resources/CoSMo_ComparativeStudyOfWebrtcOpenSourceSfusForVideoConferencing.pdf)

---

**End of Report**

**Next Steps:**
1. Evaluate use cases against decision matrix
2. Prototype preferred approach (recommend: Backend Proxy enhancement)
3. Performance testing on Jetson Nano
4. Gradual migration if pursuing backend WebRTC
