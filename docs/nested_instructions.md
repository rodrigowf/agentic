# Implementing a NestedTeamAgent with Round-Robin and Selector Modes

## NestedTeamAgent Class (AutoGen 0.5.7 Compatible)

We introduce a new `NestedTeamAgent` class (subclassing AutoGen’s `BaseChatAgent`) that wraps an inner team of agents. This allows an agent to **embed a nested sub-conversation** using either a round-robin scheme or an orchestrator (selector) scheme. The nested agent’s `on_messages` will delegate the conversation to the inner team and return the team’s final answer, including the inner dialogue for context. For streaming support, we also provide an `on_messages_stream` that yields inner events as they occur.

**`backend/nested_agent.py`:** (new file defining the NestedTeamAgent)

```python
from autogen_core import CancellationToken
from autogen_agentchat.base import Response
from autogen_agentchat.messages import BaseChatMessage, TextMessage
from autogen_agentchat.agents import BaseChatAgent  # Base class for custom agents
# Team classes and termination conditions
from autogen_agentchat.teams import RoundRobinGroupChat, SelectorGroupChat
from autogen_agentchat.conditions import MaxMessageTermination

class NestedTeamAgent(BaseChatAgent):
    """An agent that delegates to a nested team of sub-agents."""
    def __init__(self, name: str, team, description: str = "Nested team agent"):
        super().__init__(name, description=description)
        self._team = team  # Inner team (RoundRobinGroupChat or SelectorGroupChat)

    async def on_messages(
        self, messages: list[BaseChatMessage], cancellation_token: CancellationToken
    ) -> Response:
        # Run the inner team on the incoming messages (which include the new user prompt).
        result = await self._team.run(task=messages, cancellation_token=cancellation_token)
        # The inner team returns a TaskResult with a message history.
        # Extract the final answer (last message) and any intermediate messages produced by the team.
        all_msgs = result.messages
        final_msg = all_msgs[-1]  # team’s final response message
        # Inner messages: those generated by sub-agents during this turn (exclude initial input and final output)
        inner_dialog = all_msgs[len(messages):-1] if len(all_msgs) > len(messages) else []
        return Response(chat_message=final_msg, inner_messages=inner_dialog)

    async def on_messages_stream(
        self, messages: list[BaseChatMessage], cancellation_token: CancellationToken
    ):
        # Stream inner team events (each BaseChatMessage or event from sub-agents) as they occur
        async for event in self._team.run_stream(task=messages, cancellation_token=cancellation_token):
            yield event

    async def on_reset(self, cancellation_token: CancellationToken) -> None:
        # Reset the inner team conversation state
        await self._team.reset()

    @property
    def produced_message_types(self):
        # Support text messages and any message types used by sub-agents (e.g., tool events, images if any).
        # At minimum, include TextMessage as the final output type.
        return (TextMessage,)
```

In this implementation, the `NestedTeamAgent` holds a `RoundRobinGroupChat` or `SelectorGroupChat` instance. On each user query, it runs the inner team to produce a response. The outer agent’s `Response` includes the final answer (`chat_message`) and the inner conversation messages (`inner_messages`), which can be used by the UI to display or log the nested dialogue. For streaming, we directly yield events from the inner team’s `run_stream`.

## Round-Robin vs. Selector Team Modes

We support two team modes for the inner conversation:

* **RoundRobinGroupChat:** All sub-agents participate in a shared chat context, taking turns in sequence (round-robin) to respond. Each agent’s message is broadcast to the others, ensuring a consistent context. This mode is simple and rotates through the agents one by one.

* **SelectorGroupChat (Orchestrator Mode):** An **orchestrator** (backed by an LLM) reads the conversation after each message and selects which agent speaks next. This uses a ChatCompletion model to decide the next speaker, guided by a prompt template. We allow a **custom selector prompt** so the user can define how the orchestrator should choose the next agent (e.g. based on roles or current context). The default selector prompt (in AutoGen) is a role-play template, but we replace it with the user-defined prompt if provided. The orchestrator will not produce user-visible messages; it only chooses the speaking agent.

## Schema Adjustments (AgentConfig Model)

We update the Pydantic schema for agent configurations to distinguish nested team agents from standard LLM agents. In the `AgentConfig` model (e.g. in `backend/schemas.py`), we add an `agent_type` field and new fields for nested team parameters. The schema will embed sub-agent configs as a list, enabling recursive definition of agent teams.

**Changes in `AgentConfig`:** (additional fields and adjustments)

```python
from typing import Optional, List
from pydantic import BaseModel, Field

class PromptConfig(BaseModel):
    system: str
    user: str

class LLMConfig(BaseModel):
    provider: str  # e.g. "openai", "anthropic", "gemini"
    model: str
    temperature: float = 0.0
    # ... (other LLM parameters as needed)

class AgentConfig(BaseModel):
    name: str
    agent_type: str = "assistant"  # "assistant" for a single LLM agent, "nested_team" for a team agent
    # Standard agent fields:
    tools: List[str] = []
    llm: Optional[LLMConfig] = None
    prompt: Optional[PromptConfig] = None
    max_turns: int = 5
    # Optional advanced fields for single agents:
    reflect_on_tool_use: bool = False
    tool_call_loop: bool = False
    max_consecutive_auto_reply: int = 40
    # Nested team-specific fields:
    sub_agents: Optional[List["AgentConfig"]] = None  # Recursive definitions of sub-agents
    mode: Optional[str] = None                       # "round_robin" or "selector"
    orchestrator_prompt: Optional[str] = None        # Used if mode="selector"

    # You may include a validator to ensure llm/prompt are provided for assistant type 
    # and sub_agents/mode for nested_team, etc.
```

**Validation logic:** When `agent_type == "nested_team"`, the outer agent typically does not need its own LLM or prompt (the conversation is handled by sub-agents). Instead, it should have a list of `sub_agents` (each with their own `LLMConfig` and prompts) and a specified `mode` (and possibly an `orchestrator_prompt`). Conversely, for a normal LLM agent (`agent_type == "assistant"`), the fields `llm` and `prompt` must be set, while `sub_agents` can be `None`. We ensure the JSON schema reflects these requirements (e.g., via conditional logic or documentation).

## FastAPI Backend Modifications (Runner Execution)

The backend runner must be extended to **deserialize and execute the new NestedTeamAgent**. This involves constructing the inner team from the config and using our `NestedTeamAgent` wrapper when the agent type is nested. In `backend/runner.py` (inside `run_agent_ws`), add a branch for `agent_cfg.agent_type == "nested_team"`:

```python
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.models.anthropic import AnthropicChatCompletionClient
# (import other providers' client classes as needed)
from autogen_agentchat.agents import AssistantAgent
from nested_agent import NestedTeamAgent  # our new class
from autogen_agentchat.teams import RoundRobinGroupChat, SelectorGroupChat
from autogen_agentchat.conditions import MaxMessageTermination

async def run_agent_ws(agent_cfg: AgentConfig, all_tools: list[FunctionTool], websocket: WebSocket):
    # ... (initial setup and logging)
    agent_name = agent_cfg.name
    assistant = None
    try:
        if agent_cfg.agent_type == "nested_team":
            logger.info(f"Creating NestedTeamAgent for '{agent_name}' with sub-agents")
            # Instantiate each sub-agent (as AssistantAgent) from the config
            sub_agents = []
            for sub_cfg in agent_cfg.sub_agents or []:
                # Build model client for sub-agent
                prov = sub_cfg.llm.provider.lower()
                if prov == "openai":
                    model_client = OpenAIChatCompletionClient(model=sub_cfg.llm.model)
                elif prov == "anthropic":
                    model_client = AnthropicChatCompletionClient(model=sub_cfg.llm.model)
                else:
                    # Fallback to OpenAI if unknown provider (or handle other providers)
                    model_client = OpenAIChatCompletionClient(model=sub_cfg.llm.model)
                # Gather tools for this sub-agent
                sub_tools = [t for t in all_tools if t.name in sub_cfg.tools]
                # Create the sub-agent (use standard AssistantAgent for each participant)
                agent = AssistantAgent(
                    name=sub_cfg.name,
                    model_client=model_client,
                    system_message=sub_cfg.prompt.system if sub_cfg.prompt else "",
                    tools=sub_tools,
                    # You may also pass other settings like reflect_on_tool_use if needed
                )
                sub_agents.append(agent)
                logger.info(f"Initialized sub-agent '{sub_cfg.name}' for team '{agent_name}'")
            # Determine termination condition (e.g., based on max_turns)
            term_condition = MaxMessageTermination(agent_cfg.max_turns)  # stop after N messages/turns
            # Create inner team based on mode
            mode = agent_cfg.mode or "round_robin"
            if mode == "round_robin":
                logger.info(f"Using RoundRobinGroupChat for team '{agent_name}'")
                inner_team = RoundRobinGroupChat(sub_agents, termination_condition=term_condition)
            else:  # "selector" mode
                logger.info(f"Using SelectorGroupChat for team '{agent_name}'")
                # Use the first sub-agent's model client as orchestrator (or could use a dedicated one)
                orchestrator_client = sub_agents[0].model_client
                prompt = agent_cfg.orchestrator_prompt or ""  # custom orchestrator prompt (template)
                inner_team = SelectorGroupChat(
                    sub_agents, model_client=orchestrator_client,
                    termination_condition=term_condition, selector_prompt=prompt
                )
            # Wrap the inner team in our NestedTeamAgent
            assistant = NestedTeamAgent(name=agent_name, team=inner_team)
            logger.info(f"NestedTeamAgent '{agent_name}' created with {len(sub_agents)} sub-agents.")
        else:
            # ... existing logic for single-agent instantiation (LoopingAssistantAgent or AssistantAgent)
            # (as shown in the existing code snippet, e.g., using agent_cfg.tool_call_loop flag)
            # assistant = LoopingAssistantAgent(...) or AssistantAgent(...)
            pass

    except Exception as e:
        logger.exception(f"Error creating agent instance '{agent_name}': {e}")
        await send_event_to_websocket(websocket, "error", {"message": f"Failed to create agent: {e}"})
        await websocket.close()
        return

    # Proceed to run the agent (the NestedTeamAgent supports the same run_stream interface)
    cancellation_token = CancellationToken()
    # Wait for "run" message and get task (already handled above)...
    # Then:
    stream = assistant.run_stream(task=task_message_content, cancellation_token=cancellation_token)
    async for event in stream:
        # Relay events to client as usual
        event_type = event.__class__.__name__.lower()
        await send_event_to_websocket(websocket, event_type, event)
        # (The inner messages will stream as their own events; the final response from the nested agent 
        # will come as a TextMessage from the NestedTeamAgent, with inner_messages logged in the Response)
```

In the above, we construct each sub-agent using `AssistantAgent` (ensuring each has a unique `name` and its own system prompt, tools, and model client). For **RoundRobinGroupChat**, we simply provide the list of participants and a termination condition (e.g. a `MaxMessageTermination` based on `max_turns`). For **SelectorGroupChat**, we supply the participants, an LLM client for the orchestrator (here we reuse the first sub-agent’s model client for simplicity), and the `selector_prompt` (the user-defined orchestrator prompt). The orchestrator will use this prompt template to decide the next speaker each round. Finally, we wrap the configured team in `NestedTeamAgent` and proceed to run it.

> **Note:** The inner team will maintain its own conversation state across turns. After each run (or when resetting the outer agent), we call `inner_team.reset()` to clear history. The `NestedTeamAgent.on_reset` handles this. Also, ensure each sub-agent’s `name` and `description` are set, as AutoGen uses these for role identification and in the selector prompt (the `{roles}` and `{participants}` fields in the prompt template use agent names and descriptions).

## JSON Export Structure for Nested Agents

When saving or exporting a NestedTeamAgent, the configuration JSON will include the wrapper agent and its inner team definition. Below is an example JSON structure (AutoGen Studio compliant) for a nested team agent with two sub-agents in selector mode:

```json
{
  "name": "ResearchTeamAgent",
  "agent_type": "nested_team",
  "mode": "selector",
  "orchestrator_prompt": "You are an orchestrator deciding which expert should speak next based on the conversation history.",
  "max_turns": 10,
  "sub_agents": [
    {
      "name": "ExpertAI",
      "agent_type": "assistant",
      "llm": { "provider": "openai", "model": "gpt-4", "temperature": 0.7 },
      "prompt": {
        "system": "You are an expert AI assistant specializing in research.",
        "user": ""
      },
      "tools": ["wiki_browser", "calculator"],
      "max_turns": 1
    },
    {
      "name": "CriticAI",
      "agent_type": "assistant",
      "llm": { "provider": "openai", "model": "gpt-4", "temperature": 0.7 },
      "prompt": {
        "system": "You are a critical AI that analyzes and critiques the expert's answers.",
        "user": ""
      },
      "tools": [],
      "max_turns": 1
    }
  ]
}
```

In this JSON, the outer agent `ResearchTeamAgent` is of type `nested_team`. It specifies a **selector** mode with a custom `orchestrator_prompt`. The inner agents (`ExpertAI` and `CriticAI`) are listed under `sub_agents` as full agent configs (each with its own model, prompts, and tools). We set each sub-agent’s `max_turns` to 1 (since in each round they produce one message). The outer `max_turns` could define an overall cap on the number of turns in the group chat (here 10 rounds total, for example).

When this JSON is saved (via `save_agent`), it will be written with the nested structure preserved. Our Pydantic model handles the (de)serialization of the nested list of AgentConfig. This format is compatible with AutoGen Studio’s expectations, as it reflects the hierarchy of agents in a team.

## Frontend UI Updates (React)

To support creating and configuring the Nested Team Agent in the UI, we need to update the React frontend (e.g. the AgentEditor component) to handle the new agent type and nested sub-agent fields:

1. **Agent Type Selection:** Add a dropdown or toggle for selecting the agent type. For example, a `<Select>` input with options like "LLM Agent" (single assistant) and "Nested Team Agent". This selection will control which form fields are displayed.

2. **Dynamic Form Fields:**

   * If "LLM Agent" is chosen (the default/standard case), show the existing fields: Name, Tools, System Prompt, User Prompt, Provider, Model, Temperature, Max Turns, etc.
   * If "Nested Team Agent" is chosen, hide the single-agent LLM fields for the outer agent (since the outer agent itself doesn’t have its own model/prompt) and instead show:

     * A field to choose **Team Mode**: e.g., a dropdown with "Round-Robin" and "Orchestrator (Selector)".
     * If "Orchestrator" mode is selected, show a text area to input the **Orchestrator Prompt** (pre-filled with a default template or left blank for default).
     * A **Sub-Agents configuration list**: allow the user to add multiple sub-agents. For each sub-agent, display fields similar to the normal agent form (Name, Tools, System Prompt, User Prompt, LLM provider/model/temperature). These can be rendered as a repeating group of inputs. Provide an "Add Sub-agent" button to append a new blank sub-agent entry.
     * Optionally, allow reordering sub-agents (especially relevant for round-robin mode to set the speaking order).
     * The outer agent’s `max_turns` can still be set (controlling when the team stops). Sub-agents might each use a default max\_turns of 1 (one message per turn), which we can enforce in code rather than expose in UI.

3. **State Structure:** Represent the nested config in the component state. For example, `cfg.agent_type` holds the type, and if it’s `"nested_team"`, then `cfg.sub_agents` is an array of sub-agent configs (each with their own `llm`, `prompt`, etc.). Initialize `cfg.sub_agents` to an empty array or with a couple of placeholders when switching to nested mode.

4. **Saving/Updating:** Modify the save logic to send the entire `AgentConfig` JSON structure. The backend `save_agent` will serialize nested agents correctly (thanks to Pydantic). No changes to the API endpoints are needed, as the shape is still covered by the updated schema.

**Example UI code snippet (AgentEditor.jsx):** illustrating the new fields and conditional rendering for nested agents:

```jsx
// ... inside AgentEditor component render:
<FormControl sx={{ mt: 2 }}>
  <InputLabel>Agent Type</InputLabel>
  <Select
    value={cfg.agent_type}
    label="Agent Type"
    onChange={(e) => {
      const type = e.target.value;
      // Update agent_type and reset/restructure config as needed
      if (type === "nested_team") {
        setCfg({
          ...cfg,
          agent_type: "nested_team",
          // initialize sub_agents array if not present
          sub_agents: cfg.sub_agents ?? []
        });
      } else {
        setCfg({ ...cfg, agent_type: "assistant" });
      }
    }}
  >
    <MenuItem value="assistant">LLM Agent (Single)</MenuItem>
    <MenuItem value="nested_team">Nested Team Agent</MenuItem>
  </Select>
</FormControl>

{cfg.agent_type === "nested_team" ? (
  <Box sx={{ mt: 2, pl: 2, borderLeft: '2px solid #ccc' }}>
    {/* Nested Team Configuration Fields */}
    <FormControl fullWidth sx={{ mb: 2 }}>
      <InputLabel>Team Mode</InputLabel>
      <Select
        value={cfg.mode || "round_robin"}
        label="Team Mode"
        onChange={(e) => setCfg({ ...cfg, mode: e.target.value })}
      >
        <MenuItem value="round_robin">Round-Robin (Sequenced)</MenuItem>
        <MenuItem value="selector">Orchestrator (LLM-Selected)</MenuItem>
      </Select>
    </FormControl>
    {cfg.mode === "selector" && (
      <TextField 
        label="Orchestrator Prompt" 
        multiline 
        fullWidth 
        sx={{ mb: 2 }}
        value={cfg.orchestrator_prompt || ""} 
        onChange={(e) => setCfg({ ...cfg, orchestrator_prompt: e.target.value })} 
        helperText="Prompt template guiding how the orchestrator chooses the next speaker." 
      />
    )}
    <Typography variant="h6">Sub-Agents:</Typography>
    {cfg.sub_agents?.map((sub, idx) => (
      <Box key={idx} sx={{ mb: 3, pl: 2, border: '1px solid #eee', borderRadius: 1, padding: 1 }}>
        <TextField label="Sub-agent Name" value={sub.name} sx={{ mr: 1 }}
          onChange={(e) => {
            const newName = e.target.value;
            const newSubs = [...cfg.sub_agents];
            newSubs[idx] = { ...newSubs[idx], name: newName };
            setCfg({ ...cfg, sub_agents: newSubs });
          }} 
        />
        <FormControl sx={{ minWidth: 120, mr: 1 }}>
          <InputLabel>Provider</InputLabel>
          <Select value={sub.llm.provider} label="Provider"
            onChange={(e) => {
              const newProv = e.target.value;
              const newSubs = [...cfg.sub_agents];
              newSubs[idx].llm = { ...sub.llm, provider: newProv };
              setCfg({ ...cfg, sub_agents: newSubs });
            }}
          >
            <MenuItem value="openai">OpenAI</MenuItem>
            <MenuItem value="anthropic">Anthropic</MenuItem>
            <MenuItem value="gemini">Gemini</MenuItem>
          </Select>
        </FormControl>
        <TextField label="Model" value={sub.llm.model} sx={{ mr: 1 }}
          onChange={(e) => {
            const newModel = e.target.value;
            const newSubs = [...cfg.sub_agents];
            newSubs[idx].llm = { ...sub.llm, model: newModel };
            setCfg({ ...cfg, sub_agents: newSubs });
          }}
        />
        <TextField type="number" label="Temperature" value={sub.llm.temperature ?? 0} sx={{ width: 120 }}
          onChange={(e) => {
            const temp = parseFloat(e.target.value);
            const newSubs = [...cfg.sub_agents];
            newSubs[idx].llm = { ...sub.llm, temperature: temp };
            setCfg({ ...cfg, sub_agents: newSubs });
          }}
        />
        <TextField label="System Prompt" multiline fullWidth sx={{ mt: 1 }}
          value={sub.prompt?.system || ""} 
          onChange={(e) => {
            const sys = e.target.value;
            const newSubs = [...cfg.sub_agents];
            newSubs[idx].prompt = { ...sub.prompt, system: sys };
            setCfg({ ...cfg, sub_agents: newSubs });
          }}
        />
        <TextField label="User Prompt" multiline fullWidth sx={{ mt: 1 }}
          value={sub.prompt?.user || ""} 
          onChange={(e) => {
            const usr = e.target.value;
            const newSubs = [...cfg.sub_agents];
            newSubs[idx].prompt = { ...sub.prompt, user: usr };
            setCfg({ ...cfg, sub_agents: newSubs });
          }}
        />
        {/* Tool selection for sub-agent (similar multi-select as for main tools) */}
        <FormControl fullWidth sx={{ mt: 1 }}>
          <InputLabel>Tools</InputLabel>
          <Select multiple value={sub.tools || []}
            onChange={(e) => {
              const newTools = e.target.value;
              const newSubs = [...cfg.sub_agents];
              newSubs[idx].tools = newTools;
              setCfg({ ...cfg, sub_agents: newSubs });
            }}
            input={<OutlinedInput label="Tools" />}
            renderValue={(selected) => (
              <Box sx={{ display: 'flex', flexWrap: 'wrap', gap: 0.5 }}>
                {selected.map((tool) => <Chip key={tool} label={tool} />)}
              </Box>
            )}
          >
            {tools.map((t) => (
              <MenuItem key={t.name} value={t.name}>{t.name}</MenuItem>
            ))}
          </Select>
        </FormControl>
      </Box>
    ))}
    <Button variant="outlined" onClick={() => {
        // Add a new sub-agent stub
        const newSub = {
          name: `Agent${(cfg.sub_agents?.length || 0) + 1}`,
          agent_type: "assistant",
          llm: { provider: "openai", model: "", temperature: 0.0 },
          prompt: { system: "", user: "" },
          tools: []
        };
        setCfg({ ...cfg, sub_agents: [...(cfg.sub_agents || []), newSub] });
      }}
    >
      + Add Sub-agent
    </Button>
  </Box>
) : (
  // --- Standard single-agent fields (for agent_type "assistant") ---
  <>
    <FormControl>
      <InputLabel>Tools</InputLabel>
      <Select multiple value={cfg.tools} /* ...same as existing code... */ >
        {tools.map(t => <MenuItem key={t.name} value={t.name}>{t.name}</MenuItem>)}
      </Select>
    </FormControl>
    <TextField label="System Prompt" multiline value={cfg.prompt.system} onChange={...} />
    <TextField label="User Prompt" multiline value={cfg.prompt.user} onChange={...} />
    {/* ...Provider, Model, Temperature fields... */}
    <TextField type="number" label="Max Turns" value={cfg.max_turns} onChange={...} />
  </>
)}
<Button variant="contained" onClick={handleSave}>Save Agent</Button>
```

In the snippet above, we conditionally render nested team settings when `cfg.agent_type === 'nested_team'`. We allow adding sub-agents and editing each sub-agent’s fields. The **Tools** for sub-agents are handled similarly to the main agent (populating options from the loaded tool list). We also include a separate *Team Mode* selection and an *Orchestrator Prompt* field that only shows for the selector mode. The UI groups sub-agent inputs visually (indentation/border) for clarity.

With these additions, the user can configure a Nested Team Agent in the UI. When saved, the structure is sent to the backend, which uses it to instantiate the nested agent as described. This design follows the latest AutoGen 0.5.7 best practices for nested conversations – using an inner `RoundRobinGroupChat` or `SelectorGroupChat` to coordinate multiple agents, and wrapping that in a custom agent to interface with the rest of the system.

**Sources:**

1. AutoGen Teams Tutorial – *RoundRobinGroupChat* overview
2. AutoGen API Reference – *SelectorGroupChat* orchestrator description
3. AutoGen Migration Guide – Nested agent example (`NestedCountingAgent`) showing inner team usage
